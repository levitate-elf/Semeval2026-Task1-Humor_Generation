# Semeval2026-Task1-Humor_Generation

## task Aï¼šå¤šè¯­è¨€å¹½é»˜ç”Ÿæˆ

### ä»»åŠ¡æè¿°
å¯¹äºç»™å®šçš„ä¸­æ–‡ã€è‹±æ–‡ã€è¥¿ç­ç‰™è¯­çš„æ–°é—»æˆ–è€…ä¸¤ä¸ªç‰¹å®šè¯æ±‡ç»„åˆç”Ÿæˆä¸€ä¸ªç¬‘è¯

### æ•°æ®é›†
| Language | Code | Count |
|----------|------|-------|
| Chinese  | zh   | 1,000 |
| Spanish  | es   | 1,200 |
| English  | en   | 1,200 |
| **Subtotal** |      | **3,400** |
| **Total**    |      | **3,400** |

## ä¸€ã€ä½¿ç”¨ Qwen3-Max ç”Ÿæˆåˆç‰ˆç¬‘è¯

### 1. è¯»å–åŸå§‹ TSV
ä¸‰ç§è¯­è¨€çš„æ•°æ®å­˜æ”¾åœ¨ `task-a-zh.tsv / task-a-en.tsv / task-a-es.tsv` ä¸­ï¼Œæ¯æ¡æ ·æœ¬åŒ…å«ï¼š`id, word1, word2, headline` ç­‰å­—æ®µã€‚
```python
def load_all_data():
    # å¦‚æœè„šæœ¬å’Œ tsv åœ¨åŒä¸€ç›®å½•ï¼Œç›´æ¥ç”¨æ–‡ä»¶åå³å¯
    df_zh = pd.read_csv("task-a-zh.tsv", sep="\t")
    df_en = pd.read_csv("task-a-en.tsv", sep="\t")
    df_es = pd.read_csv("task-a-es.tsv", sep="\t")

    df_zh["lang"] = "zh"
    df_en["lang"] = "en"
    df_es["lang"] = "es"

    df_all = pd.concat([df_zh, df_en, df_es], ignore_index=True)
    print("æ•°æ®ç¤ºä¾‹ï¼š")
    print(df_all.head())
    return df_all
```
### 2.æ„é€ å¤šè¯­è¨€prompt
```python
def build_messages(lang, headline, word1, word2):
    """æ ¹æ®è¯­è¨€ï¼Œæ„é€  system + user æ¶ˆæ¯åˆ—è¡¨"""
    w1 = str(word1).strip()
    w2 = str(word2).strip()
    # æœ‰äº›è¡Œå¯èƒ½æ˜¯ '-' æˆ–ç©ºï¼Œå°±å½“åšâ€œæ²¡æœ‰è¯â€
    has_words = (w1 not in ["", "-", "nan"]) and (w2 not in ["", "-", "nan"])

    if lang == "zh":
        system = (
            "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿ä¸­æ–‡å¹½é»˜å†™ä½œçš„æ–‡æ¡ˆåŠ©æ‰‹ï¼Œé£æ ¼æœºæ™ºã€è½»æ¾ï¼Œ"
            "ä¸ä½ä¿—ä¸è¿‡åˆ†é»‘æš—ã€‚"
        )
        if has_words:
            user = (
                "ä¸‹é¢ç»™å‡ºä¸€æ¡æ–°é—»æ ‡é¢˜å’Œä¸¤ä¸ªå‡ ä¹ä¸ç›¸å…³çš„è¯ï¼Œè¯·ç”¨ä¸­æ–‡å†™ä¸€æ®µç®€çŸ­å¹½é»˜ï¼š\n"
                f"æ–°é—»æ ‡é¢˜ï¼š{headline}\n"
                f"è¯1ï¼š{w1}\n"
                f"è¯2ï¼š{w2}\n"
                "è¦æ±‚ï¼š\n"
                "1. è¾“å‡ºä¸€åˆ°ä¸¤å¥å®Œæ•´çš„ä¸­æ–‡å¹½é»˜è¯ï¼›\n"
                "2. å¥å­è¦å›´ç»•æ–°é—»å¤§æ„ï¼ŒåŒæ—¶å¿…é¡»åŒ…å«è¿™ä¸¤ä¸ªè¯ï¼›\n"
                "3. å…è®¸å¤¸å¼ å’Œæ¯”å–»ï¼Œä½†ä¸è¦åŒ…å«è„è¯æˆ–äººèº«æ”»å‡»ï¼›\n"
                "4. åªè¾“å‡ºæœ€åçš„å¹½é»˜å†…å®¹ï¼Œä¸è¦è§£é‡Šã€‚"
            )
        else:
            user = (
                "æŠŠä¸‹é¢è¿™æ¡æ–°é—»æ ‡é¢˜æ”¹å†™æˆä¸€å¥ç®€çŸ­çš„å¹½é»˜ä¸­æ–‡å¥å­ï¼š\n"
                f"ã€Œ{headline}ã€\n"
                "åªè¾“å‡ºä¸€å¥è¯ï¼Œä¸è¦è§£é‡Šã€‚"
            )

    elif lang == "en":
        system = (
            "You are a witty but non-offensive English joke writer. "
            "You turn serious news headlines into short humorous sentences."
        )
        if has_words:
            user = (
                "Given a news headline and two seemingly unrelated words, "
                "write one or two short humorous English sentences:\n"
                f"Headline: {headline}\n"
                f"Word 1: {w1}\n"
                f"Word 2: {w2}\n"
                "Requirements:\n"
                "1. The sentences must be in natural English and mildly sarcastic or funny;\n"
                "2. They should stay roughly on the topic of the headline and explicitly use both words;\n"
                "3. Do not output any explanations or bullet points, just the final humorous text."
            )
        else:
            user = (
                "Turn the following news headline into one short humorous English sentence:\n"
                f"\"{headline}\"\n"
                "Only output that single sentence, no explanations."
            )

    else:  # es
        system = (
            "Eres un redactor humorÃ­stico en espaÃ±ol. "
            "Tu estilo es ingenioso y ligero, sin ser ofensivo."
        )
        if has_words:
            user = (
                "A partir de un titular de noticias y dos palabras casi no relacionadas, "
                "escribe una o dos frases cortas y graciosas en espaÃ±ol:\n"
                f"Titular: {headline}\n"
                f"Palabra 1: {w1}\n"
                f"Palabra 2: {w2}\n"
                "Requisitos:\n"
                "1. La frase debe ser natural y humorÃ­stica, sin insultos;\n"
                "2. Debe mantener el tema general del titular e incluir explÃ­citamente las dos palabras;\n"
                "3. Devuelve solo el texto final del chiste, sin explicaciones ni listas."
            )
        else:
            user = (
                "Convierte el siguiente titular de noticias en una frase corta y graciosa en espaÃ±ol:\n"
                f"\"{headline}\"\n"
                "Devuelve solo esa frase, sin explicaciones."
            )

    return [
        {"role": "system", "content": system},
        {"role": "user", "content": user},
    ]
```
### 3.è°ƒç”¨ Qwen3-Max ç”Ÿæˆç¬‘è¯
```python
def gen_joke(messages):
    resp = client.chat.completions.create(
        model="qwen3-max",       
        messages=messages,
        temperature=0.8,
        top_p=0.9,
        max_tokens=128,
        extra_body={"enable_thinking": False}, 
    )
    return resp.choices[0].message.content.strip()
```
### 4. ç”Ÿæˆç»“æœç¤ºæ„
ä½¿ç”¨ Qwen3-Max å…±ç”Ÿæˆçº¦ 3,400 æ¡ å¤šè¯­è¨€å¹½é»˜æ–‡æœ¬ï¼Œä»¥ä¸‹ä¸ºéƒ¨åˆ†å¯è§†åŒ–ç¤ºä¾‹ï¼š
---
- ä¸­æ–‡ç¤ºä¾‹ï¼š
  
  ![ç¤ºä¾‹å›¾ç‰‡](./result/zh_humor.png)
- è‹±æ–‡ç¤ºä¾‹ï¼š
  ![ç¤ºä¾‹å›¾ç‰‡](./result/en_humor.png)
- è¥¿ç­ç‰™è¯­ç¤ºä¾‹ï¼š
 ![ç¤ºä¾‹å›¾ç‰‡](./result/es_humor.png)

---

## äºŒã€IMDB å½±è¯„ç»­å†™ä¸Šçš„ PPO å®éªŒ
ç”±äºå•çº¯ä¾èµ–å¤§æ¨¡å‹ zero-shot ç”Ÿæˆçš„ç¬‘è¯è´¨é‡å¹¶ä¸ç¨³å®šï¼Œåç»­è®¡åˆ’ä½¿ç”¨ PPOï¼ˆProximal Policy Optimizationï¼‰ å¯¹å°æ¨¡å‹è¿›è¡Œ RLHF é£æ ¼çš„å¾®è°ƒï¼Œä»¥æå‡å¹½é»˜ç”Ÿæˆè´¨é‡ã€‚
åœ¨çœŸæ­£ä¸Šå¹½é»˜ä»»åŠ¡ä¹‹å‰ï¼Œå…ˆåœ¨ IMDB å½±è¯„ç»­å†™ ä¸Šåšäº†ä¸€ä¸ªå®Œæ•´çš„ PPO å®éªŒï¼Œç”¨æ¥ç†Ÿæ‚‰æ•´æ¡ RLHF æµç¨‹ï¼Œå¹¶è§‚å¯Ÿå¸¸è§â€œç¿»è½¦æ¨¡å¼â€ã€‚


### 1. 1_actor.py â€” è®­ç»ƒå½±è¯„ç»­å†™ Actor
- åº•æ¨¡ï¼šEleutherAI/pythia-160m
- ç›®æ ‡ï¼šåœ¨ IMDB ä¸Šåšçº¯è¯­è¨€å»ºæ¨¡ï¼Œå¾—åˆ°ä¸€ä¸ªä¼šç»­å†™å½±è¯„çš„ Causal LMã€‚

#### æ ¸å¿ƒæ­¥éª¤ï¼š

#### ï¼ˆ1ï¼‰ç¯å¢ƒå’Œ tokenizer
```python
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-160m')
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
```
#### ï¼ˆ2ï¼‰åŠ è½½ IMDB æ•°æ®ï¼Œæ‹¼è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶åªä¿ç•™ text
```python
dataset = load_dataset('imdb')
dataset = concatenate_datasets(list((dataset.values())))
dataset = dataset.remove_columns(['label'])
```
#### ï¼ˆ3ï¼‰åŠ è½½ä¸€ä¸ª Causal LM æ¨¡å‹ï¼Œå½“ actor
```python
model_actor = AutoModelForCausalLM.from_pretrained('EleutherAI/pythia-160m').to(device)
optimizer = torch.optim.Adam(model_actor.parameters(), lr=1e-5)
```
#### ï¼ˆ4ï¼‰è®­ç»ƒå¾ªç¯ï¼šæ ‡å‡† SFT
```python
for epoch in range(10):
    for i, data in enumerate(loader):
        out = model_actor(**data)
        out.loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```


### 2. 2_critic.py â€” è®­ç»ƒæƒ…æ„Ÿ Critic / Reward Model
- ä»ç„¶ä½¿ç”¨ Pythia-160M ä½œä¸ºåº•æ¨¡ã€‚
- å°† IMDB çš„ label ä½œä¸ºæƒ…æ„Ÿæ ‡æ³¨ï¼Œè®­ç»ƒä¸€ä¸ª 0/1 æƒ…æ„Ÿæ‰“åˆ†æ¨¡å‹ï¼Œç”¨äºç»™ç»­å†™ç»“æœæ‰“ rewardã€‚

#### æ ¸å¿ƒæ­¥éª¤


#### ï¼ˆ1ï¼‰ç¯å¢ƒå’Œ tokenizer
```python
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-160m')
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
```
#### ï¼ˆ2ï¼‰åŠ è½½ IMDB æ•°æ®ï¼Œæ‹¼è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶ä¿ç•™label
```python
dataset = load_dataset('imdb')
dataset = concatenate_datasets([dataset[i] for i in ['train', 'test']])
```
#### ï¼ˆ3ï¼‰è®­ç»ƒä¸€ä¸ªæƒ…æ„Ÿè¯„åˆ†0/1ï¼ˆè´Ÿé¢/æ­£é¢ï¼‰æ¨¡å‹
```python
model_critic = AutoModelForSequenceClassification.from_pretrained(
    'EleutherAI/pythia-160m', num_labels=1).to(device)
model_critic.config.pad_token_id = tokenizer.pad_token_id
```
#### ï¼ˆ4ï¼‰è®­ç»ƒå¾ªç¯ï¼šæ ‡å‡† SFT
```python
for epoch in range(10):
    for i, data in enumerate(loader):
        out = model_actor(**data)
        out.loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```


### 3. 3_ppo.py â€” æ‰‹å†™ PPO ç®—æ³•

#### ï¼ˆ1ï¼‰å‡†å¤‡ tokenizerå’Œæ•°æ®

tokenizer åŒæ ·æ˜¯ Pythia çš„ï¼Œè¿™é‡ŒæŠŠ max_length=5ï¼Œåªå–çŸ­ prompt ç”¨æ¥é—®é—®é¢˜ã€‚
```python
def collator(data):
    data = [i['text'] for i in data]
    return tokenizer(data, padding=True, truncation=True, max_length=5, return_tensors='pt').input_ids.to(device)
```
#### ï¼ˆ2ï¼‰åŠ è½½å››ä¸ªæ¨¡å‹ï¼šactor / actor_ref / critic / critic_ref
```python
model_actor     = AutoModelForCausalLM.from_pretrained('model/actor')
model_actor_ref = AutoModelForCausalLM.from_pretrained('model/actor')

model_critic     = AutoModelForSequenceClassification.from_pretrained('model/critic', num_labels=1)
model_critic_ref = AutoModelForSequenceClassification.from_pretrained('model/critic', num_labels=1)
```
actor_refã€critic_ref æ˜¯æ—§ç­–ç•¥/æ—§ä»·å€¼çš„å†»ç»“å‚ç…§ï¼Œç”¨æ¥ç®— KLã€ç®— rewardï¼ŒçœŸå®æ›´æ–°çš„æ˜¯ model_actor å’Œ model_criticã€‚

#### ï¼ˆ3ï¼‰PPO ç›®æ ‡å‡½æ•°
ç”¨ KL + reward + advantage + clipped ratioï¼Œæ‰‹åŠ¨å®ç°äº† PPO æ›´æ–°ã€‚
å¯¹ value éƒ¨åˆ†åš clip
```python
value_clip = torch.clamp(value_new, value_old - 0.2, value_old + 0.2)
loss_vf = max( (value_new - returns)^2, (value_clip - returns)^2 )
```
å¯¹ policy åš clipped surrogate objectiveï¼š
```python
ratio = exp(prob_new - prob_old)
loss_pg1 = -advantage * ratio
loss_pg2 = -advantage * clamp(ratio, 0.8, 1.2)
loss_pg  = max(loss_pg1, loss_pg2)
```

### 4. ä½¿ç”¨ TRL é‡å†™ PPOï¼š5_trl.py

ç”¨ Hugging Face çš„ trl åº“æ¥åš PPO
#### ï¼ˆ1ï¼‰å‡†å¤‡ tokenizerå’Œæ•°æ®
tokenizer åŒæ ·æ˜¯ Pythia çš„ï¼Œè¿™é‡ŒæŠŠ max_length=5ï¼Œåªå–çŸ­ prompt ç”¨æ¥é—®é—®é¢˜ã€‚
```python
f = lambda data: {
    'input_ids': tokenizer.encode(data['text'], truncation=True, max_length=5)
}
dataset = dataset.map(f, remove_columns=dataset.column_names)
dataset = dataset.train_test_split(test_size=2000)
```
#### ï¼ˆ2ï¼‰åŠ è½½å››ä¸ªæ¨¡å‹ï¼šactor / actor_ref / critic / critic_ref
```python
model_actor     = AutoModelForCausalLM.from_pretrained('model/actor')
model_actor_ref = AutoModelForCausalLM.from_pretrained('model/actor')

model_critic     = AutoModelForSequenceClassification.from_pretrained('model/critic', num_labels=1)
model_critic_ref = AutoModelForSequenceClassification.from_pretrained('model/critic', num_labels=1)

```
actor_refã€critic_ref æ˜¯æ—§ç­–ç•¥/æ—§ä»·å€¼çš„å†»ç»“å‚ç…§ï¼Œç”¨æ¥ç®— KLã€ç®— rewardï¼ŒçœŸå®æ›´æ–°çš„æ˜¯ model_actor å’Œ model_criticã€‚
#### ï¼ˆ3ï¼‰é…ç½®PPOConfig
```python
config = PPOv2Config(
    output_dir='output_dir',
    per_device_train_batch_size=8,
    gradient_accumulation_steps=1,
    total_episodes=20_0000,
    learning_rate=5e-6,
    logging_dir='output_dir',
    run_name='run_name',
    save_strategy='no',
)
```
æœ€åˆçš„ä»£ç ä½¿ç”¨äº†è¾ƒè€çš„ PPOv2Trainer æ¥å£ï¼Œå·²ä¸å…¼å®¹æœ€æ–°ç‰ˆæœ¬çš„ trlã€‚å› æ­¤æ”¹ä¸ºä½¿ç”¨å½“å‰ç‰ˆæœ¬æä¾›çš„ PPOConfig + PPOTrainerã€‚
```python
ppo_config = PPOConfig(
    output_dir="model/ppo_trl",         # ä¿å­˜ç›®å½•
    learning_rate=5e-6,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=2,
    # PPO ç›¸å…³
    num_ppo_epochs=2,
    batch_size=8,
    mini_batch_size=2,
    total_episodes=20_0000,
)
```
#### ï¼ˆ4ï¼‰ä½¿ç”¨ PPOTrainer è¿›è¡Œè®­ç»ƒï¼š
```python
trainer = PPOTrainer(
    args=ppo_config,
    processing_class=tokenizer,     
    model=model_actor,               
    ref_model=model_actor_ref,       
    reward_model=model_critic_ref,  
    value_model=model_critic,       
    train_dataset=dataset["train"], 
    eval_dataset=dataset['test'],
)
```
20 ä¸‡ episode çš„è®­ç»ƒè€—æ—¶æ¥è¿‘**15** å°æ—¶ï¼ˆå•å¡ï¼‰ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![ç¤ºä¾‹å›¾ç‰‡](./result/trl_time.png)

### 5.æµ‹è¯• PPO / TRL æ¨¡å‹ï¼š4_test.py

4_test.py ç”¨äºåŠ è½½ PPO / TRL è®­ç»ƒåçš„æ¨¡å‹ï¼Œå¯¹éšæœºé€‰å–çš„ IMDB å¥å­è¿›è¡Œç»­å†™ï¼Œç›´è§‚æŸ¥çœ‹é£æ ¼å˜åŒ–ã€‚
#### ï¼ˆ1ï¼‰æ‰‹å†™ PPO çš„ç°è±¡
æ‰‹å†™ PPO çš„æ¨¡å‹åŸºæœ¬å­¦ä¼šäº†ä¸€ä¸ªâ€œä¸‡èƒ½å½±è¯„æ¨¡æ¿â€ï¼š
- â€œThis is the best movie I've ever seen. I love this movie. I would recommend it to everyone.â€

æ— è®º prompt æ˜¯ä»€ä¹ˆï¼Œæ¨¡å‹éƒ½å€¾å‘äºç»™å‡º æåº¦æ­£å‘ã€ç±»ä¼¼å¥å¼ çš„å¥½è¯„æ®µè½â€”â€”å…¸å‹çš„ã€Œé«˜åˆ†æ¨¡æ¿ã€æ¨¡å¼å´©å¡Œã€‚
ç”Ÿæˆç¤ºæ„:
![ç¤ºä¾‹å›¾ç‰‡](./result/ppo_output.png)

#### ï¼ˆ2ï¼‰ TRL ç‰ˆæœ¬çš„åˆå§‹é—®é¢˜ï¼šå˜æˆ â€œmovie å¤è¯»æœºâ€
åœ¨ä½¿ç”¨ TRL ç‰ˆæœ¬çš„ PPO æ—¶ï¼Œå¦‚æœä¸åŠ ä»»ä½•é™åˆ¶ï¼Œæœ€ç»ˆæ¨¡å‹å‡ ä¹é€€åŒ–æˆï¼š
- movie. movie movie movie movie movie ...

ä¸è®ºå‰ç¼€å¦‚ä½•ï¼Œéƒ½é‡å¤è¾“å‡ºé«˜é¢‘è¯ â€œmovieâ€ã€‚
ç”Ÿæˆç¤ºæ„ï¼š
![ç¤ºä¾‹å›¾ç‰‡](./result/trl_output.png)
å°è¯•åŠ è½½ä¸­é—´ checkpointï¼š
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

ckpt_path = "model/ppo_trl/checkpoint-500"  

tokenizer = AutoTokenizer.from_pretrained(ckpt_path)
model_actor = AutoModelForCausalLM.from_pretrained(ckpt_path).to(device)
```
å‘ç° 500 step é™„è¿‘å·²ç»å‡ºç°äº†æ˜æ˜¾çš„å¡‘æ€§å¡Œç¼©ï¼Œè¯´æ˜ ç­–ç•¥éå¸¸æ—©å°±è¢« reward æ¨å‘â€œé«˜é¢‘å¤è¯»â€çš„æç«¯ã€‚
åˆ†ææ˜¯reward åªçœ‹â€œè¿™æ˜¯å½±è¯„ + æœ‰ç‚¹æ­£é¢â€ï¼Œæ²¡æƒ©ç½šé‡å¤ã€æ²¡è€ƒè™‘ä¿¡æ¯é‡ï¼Œæ¨¡å‹å°±å­¦ä¼šäº†æœ€ç®€å•çš„åšæ³•ï¼šé«˜é¢‘è¯ + çœ‹èµ·æ¥åƒå½±è¯„ã€‚
#### ï¼ˆ3ï¼‰ä¿®å¤ï¼šè§£ç çº¦æŸ + å‡å°‘ episode
å¯¹ trl æ¨ç†é˜¶æ®µåŠ å…¥é˜²å¤è¯»çº¦æŸï¼š
```python
answers = model_actor.generate(
    input_ids=question,
    max_new_tokens=50,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    top_k=50,
    repetition_penalty=1.2,   # è¿™ä¸ªèƒ½æŠ‘åˆ¶ä¸€éƒ¨åˆ†å¤è¯»
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)
```
åŒæ—¶ï¼Œå°† total_episodes ä» 200_000 å¤§å¹…é™ä½åˆ° 5_000ï¼š
```python
total_episodes=5000,
```
ä¿®å¤åçš„ TRL ç»“æœæ˜æ˜¾å¥½äºâ€œmovie å¤è¯»æœºâ€ï¼Œè™½ç„¶ä»å¸¦æœ‰ä¸€å®šæ¨¡æ¿åŒ–ï¼Œä½†å¥å­è‡³å°‘å…·å¤‡åŸºæœ¬ä¿¡æ¯é‡å’Œè¿è´¯æ€§
ç”Ÿæˆç¤ºæ„ï¼š
![ç¤ºä¾‹å›¾ç‰‡](./result/trl_fix_output.png)

### 6.ä» IMDB PPO å®éªŒå¾—åˆ°çš„ç»éªŒï¼Œå¯¹å¹½é»˜ç”Ÿæˆçš„å¯å‘
IMDB å®éªŒä¸»è¦ç»™äº†ä»¥ä¸‹å‡ ç‚¹å¯ç›´æ¥è¿ç§»åˆ°å¹½é»˜ä»»åŠ¡ä¸­çš„æ•™è®­ï¼š
#### ï¼ˆ1ï¼‰å•ä¸€ Reward ææ˜“é€ æˆæ¨¡æ¿åŒ–å’Œæ¨¡å¼å¡Œç¼©
- åœ¨å½±è¯„ä»»åŠ¡ä¸­ï¼Œåªä¼˜åŒ–â€œæ­£å‘æƒ…æ„Ÿâ€è¿™ä¸€ç»´ï¼ŒPPO å¾ˆå¿«æ”¶æ•›åˆ°ã€Œç‹‚å¹ç”µå½±ã€æ¨¡æ¿ã€‚
- å¯¹åº”åˆ°å¹½é»˜ç”Ÿæˆï¼Œå¦‚æœåªç”¨ä¸€ä¸ªâ€œå¹½é»˜å¾—åˆ†â€ä½œä¸º rewardï¼Œæ¨¡å‹ä¹Ÿå¾ˆå¯èƒ½å­¦å‡ºä¸€ä¸¤ä¸ªå›ºå®šç¬‘ç‚¹ç»“æ„å¹¶ç–¯ç‹‚å¤ç”¨ã€‚
#### ï¼ˆ2ï¼‰å¹½é»˜ Reward éœ€è¦æ‹†åˆ†ä¸ºå¤šç»´ç»¼åˆæŒ‡æ ‡
åç»­è®¾è®¡ä¸­ï¼Œè®¡åˆ’è‡³å°‘åŒ…å«ï¼š
- **ç›¸å…³æ€§**ï¼šç”Ÿæˆæ–‡æœ¬ä¸æ–°é—»æ ‡é¢˜ / è¯ç»„çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå¦‚å¥å‘é‡ä½™å¼¦ï¼‰ï¼›
- **å¹½é»˜åº¦**ï¼šä¸“é—¨è®­ç»ƒçš„â€œæœ‰è¶£ / ä¸æœ‰è¶£â€åˆ†ç±»å™¨æˆ–æ‰“åˆ†æ¨¡å‹ï¼›
- **å¤šæ ·æ€§**ï¼šn-gram é‡å¤æƒ©ç½šã€é•¿åº¦æ­£åˆ™ã€è¯æ±‡å¤šæ ·åº¦ç­‰ã€‚

## äºŒã€å°è¯•PPO
åœ¨æ­£å¼å¯¹å¹½é»˜ç”Ÿæˆä»»åŠ¡å¼•å…¥ PPO ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆè¿›è¡Œä¸€è½®ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åŸºçº¿å®éªŒï¼Œä»¥éªŒè¯å°æ¨¡å‹å¯¹å¤šè¯­è¨€å¹½é»˜ä»»åŠ¡çš„å¯å­¦ä¹ æ€§
### 1.V1 åŸºçº¿ï¼šä»…ä½¿ç”¨ Qwen3-Max åˆç‰ˆç¬‘è¯åš SFT
å°† Qwen3-Max ç”Ÿæˆçš„ 3,400 æ¡å¤šè¯­è¨€ç¬‘è¯ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œå¯¹ actor è¿›è¡Œå…¨å‚æ•° SFTã€‚
è®­ç»ƒååœ¨å†…éƒ¨æ ·ä¾‹ä¸Šçš„è¡¨ç°ä¸åŸå§‹ gold çš„é£æ ¼ä¸è¡¨è¾¾é«˜åº¦æ¥è¿‘ï¼Œå­˜åœ¨æ˜æ˜¾çš„â€œå¤ç°å¼ç”Ÿæˆ / æ”¹å†™å¹…åº¦è¾ƒå°â€çš„ç°è±¡ã€‚
è¿™è¯´æ˜ï¼šä»…ä¾èµ– v1 è‡ªåŠ¨ç”Ÿæˆ goldï¼Œå¢ç›Šæœ‰é™ï¼Œæ¨¡å‹çš„â€œå¹½é»˜å¼ åŠ›â€ä¸â€œäººç±»æ®µå­æ„Ÿâ€ä»ä¸è¶³ã€‚
ç»“æœç¤ºæ„å¦‚ä¸‹ï¼š

![ç¤ºä¾‹å›¾ç‰‡](./result/actor_v1.png)
### 2. äººå·¥å¢å¼ºï¼šåŠ å…¥ 100 æ¡ä¸­æ–‡äººå·¥å¹½é»˜æ ·æœ¬

è€ƒè™‘åˆ°è‡ªåŠ¨ç”Ÿæˆ gold çš„â€œå¹½é»˜ç¨‹åº¦â€å’Œâ€œåæ§½åº¦â€æœ‰é™ï¼Œæˆ‘ä»¬åŸºäºæ•°æ®é›†ä¸­å‰ 100 æ¡ä¸­æ–‡æ ‡é¢˜ç²¾å¿ƒç­›é€‰äº†çº¦ 100 æ¡æ›´ç¬¦åˆäººç±»å¹½é»˜åå¥½çš„ç¬‘è¯ï¼Œç”¨äºæå‡è®­ç»ƒä¿¡å·çš„â€œäººç±»æ®µå­åŸºå‡†â€ã€‚
ä¸ºé¿å…å°‘é‡äººå·¥æ ·æœ¬è¢«å¤§è§„æ¨¡è‡ªåŠ¨æ•°æ®ç¨€é‡Šï¼Œæˆ‘ä»¬é‡‡ç”¨ä»¥ä¸‹ç­–ç•¥ï¼š
- å°†äººå·¥æ ·æœ¬ä¸ v1 è‡ªåŠ¨æ ·æœ¬åˆå¹¶ï¼›
- åœ¨æ•°æ®ä¸­æ–°å¢ is_human âˆˆ {0,1} æ ‡ç­¾ï¼›
- å¼ºåˆ¶åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•ï¼š
  - è®­ç»ƒé›†ï¼šåŒ…å«äººå·¥æ ·æœ¬ + è‡ªåŠ¨æ ·æœ¬
  - æµ‹è¯•é›†ï¼šä»…åŒ…å«éäººå·¥æ ·æœ¬
    ä»¥åŒæ—¶ä¿è¯è®­ç»ƒå—ç›Šäºé«˜è´¨é‡äººç±»å¹½é»˜ä¿¡å·ï¼Œåˆé¿å…è¯„ä¼°é˜¶æ®µå‘ç”Ÿâ€œè®°å¿†å¼ä¹è§‚åå·®â€ã€‚
    
è¯¥ç‰ˆæœ¬è®°ä¸º V11ï¼ˆv1 + human boostï¼‰ã€‚

ç»“æœç¤ºæ„å¦‚ä¸‹ï¼š
![ç¤ºä¾‹å›¾ç‰‡](./result/actor_v12.png)
### 3. å†å‡çº§ï¼šFew-shot è¾…åŠ© Qwen3-Max ç”Ÿæˆ V2 Gold (Version1)
ä¸ºäº†è¿›ä¸€æ­¥æå‡è‡ªåŠ¨ gold çš„â€œäººç±»æ®µå­å‘³â€ï¼Œæˆ‘ä»¬åœ¨ V11 çš„åŸºç¡€ä¸Šï¼Œå¼•å…¥ few-shot é£æ ¼ç¤ºä¾‹ä¸æ›´åâ€œèªæ˜çš„å/ç°å®åæ§½â€çš„å†™ä½œçº¦æŸï¼Œå†è°ƒç”¨ Qwen3-Max å¯¹ v1 è‰ç¨¿è¿›è¡ŒäºŒæ¬¡å‡æ¡£ï¼Œç”Ÿæˆ V2 ç‰ˆæœ¬ goldã€‚
è¿™ä¸€é˜¶æ®µçš„ç›®æ ‡ä¸æ˜¯è®©æ¨¡å‹å˜â€œæ›´å®‰å…¨æ›´ä¸­æ€§â€ï¼Œè€Œæ˜¯ï¼š
- é¿å…é™ˆè¿°å¥å¼å½’çº³ï¼›
- å¢åŠ åå·®ã€è®½åˆºã€æ„å¤–è½¬æŠ˜ä¸ç²¾å‡†åæ§½ï¼›
- è®©è‡ªåŠ¨æ ·æœ¬æ›´æ¥è¿‘äººç±»å†™ä½œçš„â€œå¹½é»˜â€
  
éƒ¨åˆ†ç»“æœå¦‚å›¾æ‰€ç¤º

![ç¤ºä¾‹å›¾ç‰‡](./result/v2_output.png)
### 4. æ•°æ®æ‰©å¢ï¼šDeepSeek-V3.1 è¾…åŠ©ç”Ÿæˆ V2 Gold (Version 2)

è€ƒè™‘åˆ°ä»…ä¾èµ– Qwen å•ä¸€æ¨¡å‹ç”Ÿæˆçš„ V2 æ•°æ®é‡çº§ï¼ˆçº¦ 1,700 æ¡ï¼‰åœ¨è®­ç»ƒæ—¶å¯èƒ½å¯¼è‡´æ¨¡å‹å¯¹ç‰¹å®šå¥å¼è¿‡æ‹Ÿåˆï¼Œä¸”æ•´ä½“æ•°æ®ä¸°å¯Œåº¦ä¸è¶³ï¼Œæˆ‘ä»¬å¼•å…¥ **DeepSeek-V3.1** ä½œä¸ºç¬¬äºŒä¸ªå¼ºåŠ› Teacher Model è¿›è¡Œæ•°æ®æ‰©å¢ã€‚

**ç­–ç•¥æ ¸å¿ƒï¼š**
* **Prompt è¿ç§»**ï¼šå¤ç”¨ Qwen V2 é˜¶æ®µéªŒè¯æœ‰æ•ˆçš„ Few-shot Promptï¼Œç¡®ä¿é£æ ¼å¯¹é½ã€‚
* **æ¨¡å‹å¼‚æ„æ€§**ï¼šåˆ©ç”¨ DeepSeek ä¸ Qwen åœ¨è®­ç»ƒè¯­æ–™å’Œæ¨ç†é£æ ¼ä¸Šçš„å·®å¼‚ï¼Œå¼•å…¥æ›´å¤šæ ·åŒ–çš„å¹½é»˜è¡¨è¾¾å¥å¼ï¼Œé˜²æ­¢ Actor é™·å…¥å•ä¸€æ¨¡å‹çš„â€œè¯­è¨€æŒ‡çº¹â€ä¸­ã€‚

é€šè¿‡æ­¤æ­¥éª¤ï¼Œæˆ‘ä»¬é¢å¤–è·å¾—äº†ä¸€æ‰¹é«˜è´¨é‡çš„åˆæˆæ•°æ®ï¼ˆè®°ä¸º `V2_DS`ï¼‰ï¼Œæœ‰æ•ˆæ‰©å……äº†é«˜ä»·å€¼æ ·æœ¬æ± ã€‚
éƒ¨åˆ†ç»“æœå¦‚å›¾æ‰€ç¤º

![ç¤ºä¾‹å›¾ç‰‡](./result/v2ds_output.png)
### 5. ç»ˆæèåˆï¼šæ„å»º V13 æ··åˆæ•°æ®é›† (SFT Final)

åŸºäºâ€œ**æ•°æ®è´¨é‡å†³å®š SFT ä¸Šé™**â€çš„åŸåˆ™ï¼Œå¯¹ç°æœ‰æ‰€æœ‰æ•°æ®æºè¿›è¡Œäº†æ¸…æ´—ã€è¯„åˆ†ä¸åˆ†å±‚åŠ æƒï¼Œæ„å»ºäº†æœ€ç»ˆç”¨äº Actor SFT è®­ç»ƒçš„ **V13 æ•°æ®é›†**ã€‚

#### (1) æ•°æ®æ„æˆç­–ç•¥
é‡‡ç”¨äº† **â€œç²¾è‹±åŠ æƒ + ä¸»åŠ›å…¨æ”¶ + åŠ£è´¨æå°–â€** çš„æ··åˆç­–ç•¥ï¼š

| æ•°æ®æ¥æº | åŸå§‹æ•°é‡ | å¤„ç†ç­–ç•¥ | æœ€ç»ˆè´¡çŒ® (çº¦) | è§’è‰²å®šä½ |
| :--- | :--- | :--- | :--- | :--- |
| **Human (äººå·¥å¢å¼º)** | 100 | **è¿‡é‡‡æ · (Upsample) x 5** | 500 | **é£æ ¼é”šç‚¹** (æ ¸å¿ƒäººç±»å¹½é»˜æ„Ÿ) |
| **V2 (Qwen ç‰ˆ)** | ~3,400 | å…¨é‡ä¿ç•™ | 3,400 | **ä¸»åŠ›æ•°æ®** (é«˜è´¨é‡åˆæˆ) |
| **V2 (DeepSeek ç‰ˆ)** | ~3,400 | å…¨é‡ä¿ç•™ | 1,700 | **ä¸»åŠ›æ•°æ®** (å¤šæ ·æ€§è¡¥å……) |
| **V12 (åŸå§‹è‡ªåŠ¨)** | ~3,000+ | **æ¸…æ´— + è¯„åˆ†ç­›é€‰ Top 500** | 500 | **æ³›åŒ–è¡¥å……** (ä¿ç•™é€»è¾‘é€šé¡ºçš„åŸºç¡€æ ·æœ¬) |
| **Total** | - | - | **~7,800** | **V13 æœ€ç»ˆè®­ç»ƒé›†** |

#### (2) å…³é”®æ¸…æ´—ä¸ç­›é€‰æµç¨‹
ä¸ºäº†é˜²æ­¢â€œåƒåœ¾è¿›ï¼Œåƒåœ¾å‡ºâ€ï¼Œæˆ‘ä»¬åœ¨åˆå¹¶å‰å¯¹ V12 ç­‰åŸå§‹æ•°æ®æ‰§è¡Œäº†ä¸¥æ ¼çš„æ¸…æ´—æµæ°´çº¿ï¼š

* **å¼ºåŠ›å»å°¾**
    é’ˆå¯¹è‡ªåŠ¨ç”Ÿæˆæ•°æ®ä¸­å¸¸è§çš„ Prompt æ³„æ¼é—®é¢˜ï¼Œç¼–å†™æ­£åˆ™è„šæœ¬è¿›è¡Œâ€œæå¤´å»å°¾â€ï¼Œç§»é™¤å¦‚ `### Instruction:`ã€`User:`ã€`<|im_end|>` ç­‰æ®‹ç•™å­—ç¬¦ï¼Œç¡®ä¿ `joke` å­—æ®µçº¯å‡€ã€‚

* **è´¨é‡æ‰“åˆ†**
    è®¾è®¡å¤šç»´è¯„åˆ†å…¬å¼ï¼š
    > `Score = å…³é”®è¯è¦†ç›–(2åˆ†) + é•¿åº¦é€‚ä¸­(1åˆ†) + æ ‡ç‚¹å®Œæ•´(1åˆ†) + å™äº‹ç»“æ„å¥–åŠ±(1åˆ†)`
    
    å¯¹äº V12 æ•°æ®ï¼Œä»…ä¿ç•™å¾—åˆ†æœ€é«˜ä¸”é€»è¾‘å®Œæ•´çš„ **Top 500** æ¡ï¼Œå‰”é™¤çŸ­æ–‡æœ¬å’Œä¸¥é‡å¹»è§‰çš„æ ·æœ¬ã€‚

* **ID é‡æ’ä¸æ ¼å¼åŒ–**
    å°†æ‰€æœ‰æ¥æºçš„æ•°æ®ç»Ÿä¸€ ID æ ¼å¼ï¼ˆå¦‚ `zh_0001`ï¼‰ï¼Œå¹¶æ‰“ä¹±é¡ºåºï¼ˆShuffleï¼‰ï¼Œé˜²æ­¢æ¨¡å‹è®­ç»ƒæ—¶äº§ç”Ÿæ¬¡åºåå·®ã€‚

#### ï¼ˆ3ï¼‰sftéƒ¨åˆ†ç»“æœå¦‚å›¾æ‰€ç¤º

![ç¤ºä¾‹å›¾ç‰‡](./result/v13_output.png)


### 6.æ„é€ æ··åˆå¥–åŠ±æ¨¡å‹
ä¸ºäº†åœ¨ PPO é˜¶æ®µæœ‰æ•ˆæŒ‡å¯¼æ¨¡å‹ä¼˜åŒ–ï¼ŒåŒæ—¶é¿å…â€œå¹½é»˜æ„Ÿâ€çš„ä¸»è§‚æ€§å¯¼è‡´æ¨¡å‹è·‘åï¼Œè®¾è®¡äº†ä¸€å¥— â€œæ¨¡å‹æ‰“åˆ† + è§„åˆ™çº¦æŸâ€ çš„æ··åˆå¥–åŠ±ç³»ç»Ÿã€‚

#### ï¼ˆ1ï¼‰è®­ç»ƒæ•°æ®æ„é€ ï¼šåŸºäºé˜¶æ¢¯çš„ Pairwise Ranking
ä¸éœ€è¦é¢å¤–çš„äººå·¥æ ‡æ³¨ï¼Œè€Œæ˜¯ç›´æ¥åˆ©ç”¨ V13 æ•°æ®é›†çš„å¤©ç„¶è´¨é‡é˜¶æ¢¯ **(Data Tiers)** æ¥è‡ªåŠ¨æ„å»º **(Chosen, Rejected)** åå¥½å¯¹ï¼š
| Pair ç±»å‹ | Chosen (èƒœè€…) | Rejected (è´¥è€…) | å­¦ä¹ ç›®æ ‡ |
| :--- | :--- | :--- | :--- |
| **é£æ ¼å¯¹é½** | **Human (Tier 0)** | V2 Synthetic (Tier 1) | å­¦ä¹ äººç±»ç‰¹æœ‰çš„â€œè¨€å¤–ä¹‹æ„â€å’Œâ€œå¹½é»˜å¼ åŠ›â€ |
| **é€»è¾‘ä¼˜åŒ–** | **V2 Synthetic (Tier 1)** | V12 Auto (Tier 2) | å­¦ä¹ æ›´ä¸¥è°¨çš„å™äº‹ç»“æ„ï¼Œæ‹’ç»æµæ°´è´¦ |
| **ç¡¬çº¦æŸæ³¨å…¥** | **V12 Auto (Tier 2)** | **æ„é€ è´Ÿæ ·æœ¬ (Tier 3)** | å­¦ä¹ å¿…é¡»åŒ…å«å…³é”®è¯ (é€šè¿‡éšæœºå‰”é™¤å…³é”®è¯æ„é€ è´Ÿæ ·æœ¬) |

#### (2) æ¨¡å‹é€‰å‹ï¼šmDeBERTa-v3
ç›¸æ¯”äº Decoder-only çš„ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬é€‰æ‹© **Encoder-only** æ¶æ„ä½œä¸º Reward Model çš„åŸºåº§ï¼š
* **æ¨¡å‹**ï¼š`microsoft/mdeberta-v3-base` (å¤šè¯­è¨€ç‰ˆ)
* **ä¼˜åŠ¿**ï¼šåŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼ˆBidirectional Attentionï¼‰èƒ½æ›´å¥½åœ°ç†è§£ Prompt ä¸ Joke ä¹‹é—´çš„ä¸Šä¸‹æ–‡é€»è¾‘å…³è”ï¼Œä¸”åœ¨ PPO é‡‡æ ·é˜¶æ®µæ¨ç†é€Ÿåº¦æ›´å¿«ã€‚
```python
import os
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from trl import RewardTrainer, RewardConfig
from datasets import load_from_disk

# =================é…ç½®=================
# å¿…é¡»ç”¨ mDeBERTa (Multilingual)
MODEL_NAME = "microsoft/mdeberta-v3-base" 
OUTPUT_DIR = "model/humor_reward_model_v1"
DATA_PATH = "data/reward_data_v13"
# =====================================

def train_rm():
    print(f"ğŸš€ Loading Model: {MODEL_NAME}")
    
    # 1. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    # 2. åŠ è½½æ¨¡å‹
    # num_labels=1 è¡¨ç¤ºè¾“å‡ºä¸€ä¸ªæ ‡é‡åˆ†æ•°
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME, 
        num_labels=1,
        problem_type="regression", # å†…éƒ¨è§†ä¸ºå›å½’ï¼Œä½†åœ¨ RewardTrainer é‡Œæ˜¯ç”¨ ranking loss
        trust_remote_code=True
    )
    
    # 3. åŠ è½½æ•°æ®
    dataset = load_from_disk(DATA_PATH)
    
    # 4. æ•°æ®é¢„å¤„ç†å‡½æ•°
    # TRL çš„ RewardTrainer ä¼šè‡ªåŠ¨å¤„ç† chosen/rejected çš„ tokenize
    # æˆ‘ä»¬åªéœ€è¦å‘Šè¯‰å®ƒæŠŠ prompt å’Œ response æ‹¼èµ·æ¥
    def preprocess_function(examples):
        new_examples = {
            "input_ids_chosen": [],
            "attention_mask_chosen": [],
            "input_ids_rejected": [],
            "attention_mask_rejected": [],
        }
        for prompt, chosen, rejected in zip(examples["prompt"], examples["chosen"], examples["rejected"]):
            # æ„é€  DeBERTa è¾“å…¥: [CLS] Prompt [SEP] Response [SEP]
            tokenized_chosen = tokenizer(prompt, chosen, truncation=True, max_length=512)
            tokenized_rejected = tokenizer(prompt, rejected, truncation=True, max_length=512)
            
            new_examples["input_ids_chosen"].append(tokenized_chosen["input_ids"])
            new_examples["attention_mask_chosen"].append(tokenized_chosen["attention_mask"])
            new_examples["input_ids_rejected"].append(tokenized_rejected["input_ids"])
            new_examples["attention_mask_rejected"].append(tokenized_rejected["attention_mask"])
            
        return new_examples

    print("ğŸ”„ Tokenizing data...")
    tokenized_ds = dataset.map(preprocess_function, batched=True, remove_columns=dataset["train"].column_names)

    # 5. è®­ç»ƒå‚æ•°
    training_args = RewardConfig(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=8, # DeBERTa base å¾ˆå°ï¼Œæ˜¾å­˜å¤Ÿå¯ä»¥å¼€å¤§
        gradient_accumulation_steps=4,
        num_train_epochs=2,            # RM å¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œ1-2 epoch è¶³å¤Ÿ
        learning_rate=2e-5,            # Encoder æ¨¡å‹é€šå¸¸å¯ä»¥ç”¨å¤§ä¸€ç‚¹çš„ LR
        fp16=True,                     # å¼€å¯æ··åˆç²¾åº¦
        logging_steps=50,
        eval_strategy="steps",
        eval_steps=200,
        save_strategy="steps",
        save_steps=200,
        max_length=512,
        report_to="tensorboard",
        remove_unused_columns=False,
    )

    # 6. Trainer
    trainer = RewardTrainer(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        train_dataset=tokenized_ds["train"],
        eval_dataset=tokenized_ds["test"],
    )

    print("ğŸ”¥ Start Training Reward Model...")
    trainer.train()
    
    trainer.save_model(OUTPUT_DIR)
    print(f"âœ… Reward Model Saved to {OUTPUT_DIR}")

if __name__ == "__main__":
    train_rm()
```

#### (3) æ··åˆå¥–åŠ±å…¬å¼ (The Hybrid Reward Function)
åœ¨ PPO è¿‡ç¨‹ä¸­ï¼Œæœ€ç»ˆçš„ Reward ä¸ä»…ä»…ä¾èµ–ç¥ç»ç½‘ç»œçš„æ‰“åˆ†ï¼Œè€Œæ˜¯ä¸‰ä¸ªç»´åº¦çš„åŠ æƒå’Œï¼š

$$R_{total} = R_{quality} + R_{rule} + R_{relevance}$$
* **$R_{quality}$ (ç¥ç»ç½‘ç»œæ‰“åˆ†)**ï¼š
    ç”± mDeBERTa ç»™å‡ºã€‚è¡¡é‡æ–‡æœ¬çš„æµç•…åº¦ã€å¹½é»˜æ„Ÿå’Œé£æ ¼å¥‘åˆåº¦ã€‚
* **$R_{rule}$ (è§„åˆ™ç¡¬çº¦æŸ)**ï¼š
    é’ˆå¯¹è‹±æ–‡/è¥¿ç­ç‰™æ–‡å®¹æ˜“å‡ºç°çš„â€œå…³é”®è¯å¹»è§‰â€é—®é¢˜ã€‚é€šè¿‡ Regex å¼ºåˆ¶æ£€æµ‹ï¼š
    * **Missed Keywords Penalty**ï¼šè‹¥æœªåŒ…å«æŒ‡å®šå…³é”®è¯ï¼Œç›´æ¥ç»™äºˆé‡ç½šï¼ˆå¦‚ `-5.0`ï¼‰ã€‚
    * **Repetition Penalty**ï¼šè‹¥æ£€æµ‹åˆ°å¤è¯»æœºæ¨¡å¼ï¼ˆå¦‚é‡å¤å•è¯ï¼‰ï¼Œç»™äºˆæƒ©ç½šã€‚
* **$R_{relevance}$ (è¯­ä¹‰ç›¸å…³æ€§)**ï¼š
    ä½¿ç”¨è½»é‡çº§ Embedding æ¨¡å‹è®¡ç®— `CosineSimilarity(Headline, Joke)`ã€‚é˜²æ­¢æ¨¡å‹ä¸ºäº†å¹½é»˜è€Œå®Œå…¨è„±ç¦»æ–°é—»ä¸»é¢˜ï¼ˆè·‘é¢˜ï¼‰ã€‚
